{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJyNkiErhgvJJMkgrYzrR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etuckerman/SPRK/blob/main/groq_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from google.colab import userdata\n",
        "\n",
        "# # Store your API key as a secret in Colab userdata\n",
        "# api_key = '----------------'\n",
        "# secret_name = 'GROQ_API_KEY'  # Choose a name for your secret\n",
        "\n",
        "# # Set the API key directly as an environment variable\n",
        "# os.environ[\"GROQ_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "vvFuLDd0tEHd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq markdown Latex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbNDT7lgq9CD",
        "outputId": "ad0f82e9-7099-42ee-eb70-4cc998224f37"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (3.7)\n",
            "Collecting Latex\n",
            "  Downloading latex-0.7.0.tar.gz (6.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Collecting tempdir (from Latex)\n",
            "  Downloading tempdir-0.7.1.tar.gz (5.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting data (from Latex)\n",
            "  Downloading data-0.4.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from Latex) (1.0.0)\n",
            "Collecting shutilwhich (from Latex)\n",
            "  Downloading shutilwhich-1.1.0.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from data->Latex) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from data->Latex) (4.4.2)\n",
            "Collecting funcsigs (from data->Latex)\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: Latex, data, shutilwhich, tempdir\n",
            "  Building wheel for Latex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Latex: filename=latex-0.7.0-py3-none-any.whl size=7588 sha256=6459eddfc9ca0f13a99a46461e99f292f765de8ef2d3082bdb59b92f74f5f5d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/b3/95/f4b45b5116d4585893cdcb2ac7c07614a59fb047c754c4651a\n",
            "  Building wheel for data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7227 sha256=9a4f86e64a6c9db70d81db525d8bd7f42a816016883596bad6282efa6e9be945\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/d3/10/d5fe9bc9dcb197ea289baccca92a25f2f95135235a92ca1b11\n",
            "  Building wheel for shutilwhich (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shutilwhich: filename=shutilwhich-1.1.0-py3-none-any.whl size=2766 sha256=165a80c4ef5ba9a3ac5b8bb5d8b674dd69cec22b999f4ef2829664f0ffe6071b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/49/7a/3997889a5643ddb4a1d21692c6916fd2fc482965211d9a3ca5\n",
            "  Building wheel for tempdir (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tempdir: filename=tempdir-0.7.1-py3-none-any.whl size=2196 sha256=55928f25e11503b586c6f81cbaad5f4cab13f36f33bf2776c4c745873de153ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/67/0b/519393cca63cd89cea554a371cc88a4c439c2d31804e9a9ed7\n",
            "Successfully built Latex data shutilwhich tempdir\n",
            "Installing collected packages: tempdir, shutilwhich, funcsigs, data, Latex\n",
            "Successfully installed Latex-0.7.0 data-0.4 funcsigs-1.0.2 shutilwhich-1.1.0 tempdir-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.models.list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MivE20JsXrS",
        "outputId": "70602499-bd59-459e-9730-4f9d1d4c7dca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelListResponse(data=[Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None), Model(id='llama-3.2-3b-preview', created=1727224290, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='mixtral-8x7b-32768', created=1693721698, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='llama-3.2-11b-vision-preview', created=1727226869, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='llama-3.2-90b-vision-preview', created=1727226914, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.2-1b-preview', created=1727224268, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-specdec', created=1733505017, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=32768, public_apps=None), Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None)], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "LGhef7yLqA-8",
        "outputId": "67f78ee5-d646-4a2b-c474-5239db8b171c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The image is a mathematical equation set, consisting of five lines of text in a black font, each containing a variable and a constant. The first line reads \"a + b + c = 4\", the second line reads \"a^2 + b^2 + c^2 = 10\", the third line reads \"a^3 + b^3 + c^3 = 22\", the fourth line reads \"a^4 + b^4 + c^4 = ?\", and the fifth line is a question mark, indicating that the value of this equation needs to be determined.\n\nTo find the value of the fifth equation, we can analyze the pattern of the equations. The first equation is a simple sum, the second line is a sum of squares, the third line is a sum of cubes, and the fourth line is a sum of fourth powers. We can use this pattern to infer the value of the fifth line. Unfortunately, without additional information or context, it is not possible to determine the value of the fifth equation based solely on the pattern of the previous lines. Therefore, the value of the fifth equation cannot be determined with the given information.\n\n**Answer:**\nThe value of the fifth equation cannot be determined with the given information."
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import base64\n",
        "from groq import Groq\n",
        "import markdown\n",
        "from IPython.display import display, Latex\n",
        "\n",
        "\n",
        "# Set up the API client\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "# Function to encode the image to base64\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# Path to image\n",
        "image_path = \"math.jpg\"\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "# Sending image and text as a single user message\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"llama-3.2-11b-vision-preview\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What is this image?\"},\n",
        "\n",
        "\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=2048,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "\n",
        "image_description = chat_completion.choices[0].message.content\n",
        "# print(response)\n",
        "\n",
        "display(Markdown(image_description))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second model: Use the description and the user prompt to generate a detailed response\n",
        "user_prompt = \"What is the result of the math equation in the image?\"\n",
        "\n",
        "# Send to second model\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": f\"Based on this image data: {image_description}, answer the question: {user_prompt}. Respond in markdown format. Be concise but accurate.\"},\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=2048,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "# Get and display the response from the second model\n",
        "response = chat_completion.choices[0].message.content\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "N3NnxY505RgN",
        "outputId": "d12e2a2a-d9eb-4ca3-9651-f9768ed2448a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<think>\nOkay, so I have this problem with four equations involving variables a, b, and c. The equations are:\n\n1. a + b + c = 4\n2. a² + b² + c² = 10\n3. a³ + b³ + c³ = 22\n4. a⁴ + b⁴ + c⁴ = ?\n\nI need to find the value of the fourth equation, which is the sum of the fourth powers of a, b, and c. \n\nI remember that there are some formulas and methods in algebra that relate sums of powers, like Newton's identities, which connect power sums to elementary symmetric sums. Maybe I can use those to find the fourth power sum.\n\nFirst, I think I should find the elementary symmetric sums: s1, s2, and s3, where:\n- s1 = a + b + c\n- s2 = ab + ac + bc\n- s3 = abc\n\nI know s1 from the first equation: s1 = 4.\n\nTo find s2, I can use the identity that relates the sum of squares to s1 and s2. The formula is:\na² + b² + c² = s1² - 2s2\n\nPlugging in the known values:\n10 = 4² - 2s2\n10 = 16 - 2s2\n2s2 = 16 - 10\n2s2 = 6\ns2 = 3\n\nNow, to find s3, I can use the identity for the sum of cubes:\na³ + b³ + c³ = s1³ - 3s1s2 + 3s3\n\nWe know a³ + b³ + c³ = 22, so:\n22 = 4³ - 3*4*3 + 3s3\n22 = 64 - 36 + 3s3\n22 = 28 + 3s3\n3s3 = 22 - 28\n3s3 = -6\ns3 = -2\n\nNow, I can use Newton's identities to find the fourth power sum. The general formula for the sum of the nth powers, P_n, is:\nP_n = s1*P_{n-1} - s2*P_{n-2} + s3*P_{n-3}\n\nWe need P_4, and we have P_1 = 4, P_2 = 10, P_3 = 22.\n\nApplying the formula for n=4:\nP_4 = s1*P_3 - s2*P_2 + s3*P_1\nP_4 = 4*22 - 3*10 + (-2)*4\nP_4 = 88 - 30 - 8\nP_4 = 50\n\nWait, so does that mean the sum of the fourth powers is 50? That seems straightforward, but I should double-check my calculations.\n\nLet me recalculate P_4 step by step:\n- 4 * 22 = 88\n- 3 * 10 = 30\n- 3 * 10 is subtracted, so 88 - 30 = 58\n- (-2)*4 = -8\n- 58 - 8 = 50\n\nYes, that seems correct. Therefore, the value of a⁴ + b⁴ + c⁴ should be 50.\n</think>\n\nThe value of the fourth equation, \\(a^4 + b^4 + c^4\\), is determined by using Newton's identities to relate the sums of powers to elementary symmetric sums. Calculating step-by-step, we find that \\(a^4 + b^4 + c^4 = 50\\).\n\n**Answer:**\nThe result of the math equation in the image is \\(\\boxed{50}\\)."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set up the second agent's message\n",
        "# formatting_agent_message = f\"\"\"\n",
        "# You are an AI agent designed to fix formatting in model responses. The model has provided the following response, but it needs formatting corrections.\n",
        "\n",
        "# Here is the response:\n",
        "# {response}\n",
        "\n",
        "# Your task is to identify the response and format it correctly. If there is math equations, use Latex. Use Markdown elsewhere.\n",
        "# \"\"\"\n",
        "\n",
        "# # Send the LaTeX response to the formatting agent\n",
        "# formatting_completion = client.chat.completions.create(\n",
        "#     model=\"llama-3.2-11b-vision-preview\",  # Or any other suitable model\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": [\n",
        "#                 {\"type\": \"text\", \"text\": formatting_agent_message}\n",
        "#             ]\n",
        "#         }\n",
        "#     ],\n",
        "#     temperature=1,\n",
        "#     max_completion_tokens=2048,\n",
        "#     top_p=1,\n",
        "#     stream=False,\n",
        "#     stop=None,\n",
        "# )\n",
        "\n",
        "# # Get the corrected response\n",
        "# corrected_response = formatting_completion.choices[0].message.content\n",
        "\n",
        "# # Display the corrected LaTeX in Markdown format\n",
        "# # display(Markdown(corrected_response))\n",
        "# display(Latex(corrected_response))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SQOQ23Ywyl9Q",
        "outputId": "03acaa0b-0864-4b33-9d77-8ec5ba789bbd"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "**Problem-Solving with the Attached Image**\n================================================\n\nNotably, the attached image depicts a single pickle.\n\n### Solution Approach\n\nThere is no apparent mathematical or computational problem to solve in this image, as it represents a non-ambiguous visual representation of a standard pickle (likely a dill pickle). It does not elicit a specific task or problem to be resolved, and its purpose appears to be illustrating or showcasing a pickle.\n\nNote: Since there are no math equations in this response, I have used Markdown formatting as required."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1A3ZIPUbqQaV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}