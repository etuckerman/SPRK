{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5LyBXrqK/284f9P6N6ISk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etuckerman/SPRK/blob/main/groq_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# # Store API keys as a secret in Colab userdata\n",
        "# api_key = '----------------'\n",
        "# secret_name = 'GROQ_API_KEY'\n",
        "\n",
        "# # Set the API key directly as an environment variable\n",
        "# os.environ[\"GROQ_API_KEY\"] = api_key\n",
        "##########################################################################\n",
        "# api_key = '-----------------'\n",
        "# secret_name = 'LLAMAPARSE_API_KEY'\n",
        "\n",
        "# # Set the API key directly as an environment variable\n",
        "# os.environ[\"LLAMAPARSE_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "vvFuLDd0tEHd"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq markdown Latex llama_parse nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbNDT7lgq9CD",
        "outputId": "ffff68e8-d17a-4356-a7ae-88f48002a896"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (3.7)\n",
            "Requirement already satisfied: Latex in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Collecting llama_parse\n",
            "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: tempdir in /usr/local/lib/python3.11/dist-packages (from Latex) (0.7.1)\n",
            "Requirement already satisfied: data in /usr/local/lib/python3.11/dist-packages (from Latex) (0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from Latex) (1.0.0)\n",
            "Requirement already satisfied: shutilwhich in /usr/local/lib/python3.11/dist-packages (from Latex) (1.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama_parse) (8.1.8)\n",
            "Collecting llama-index-core>=0.11.0 (from llama_parse)\n",
            "  Downloading llama_index_core-0.12.15-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama_parse) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (3.11.11)\n",
            "Collecting dataclasses-json (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (11.1.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (4.67.1)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (1.17.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from data->Latex) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from data->Latex) (4.4.2)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.11/dist-packages (from data->Latex) (1.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (1.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama_parse) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama_parse) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama_parse) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama_parse) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama_parse) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.11.0->llama_parse) (24.2)\n",
            "Downloading llama_parse-0.5.20-py3-none-any.whl (16 kB)\n",
            "Downloading llama_index_core-0.12.15-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: filetype, dirtyjson, mypy-extensions, marshmallow, typing-inspect, tiktoken, dataclasses-json, llama-index-core, llama_parse\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-index-core-0.12.15 llama_parse-0.5.20 marshmallow-3.26.0 mypy-extensions-1.0.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.models.list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MivE20JsXrS",
        "outputId": "70602499-bd59-459e-9730-4f9d1d4c7dca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelListResponse(data=[Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None), Model(id='llama-3.2-3b-preview', created=1727224290, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='mixtral-8x7b-32768', created=1693721698, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='llama-3.2-11b-vision-preview', created=1727226869, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='llama-3.2-90b-vision-preview', created=1727226914, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.2-1b-preview', created=1727224268, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-specdec', created=1733505017, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=32768, public_apps=None), Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None)], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "LGhef7yLqA-8",
        "outputId": "67f78ee5-d646-4a2b-c474-5239db8b171c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The image is a mathematical equation set, consisting of five lines of text in a black font, each containing a variable and a constant. The first line reads \"a + b + c = 4\", the second line reads \"a^2 + b^2 + c^2 = 10\", the third line reads \"a^3 + b^3 + c^3 = 22\", the fourth line reads \"a^4 + b^4 + c^4 = ?\", and the fifth line is a question mark, indicating that the value of this equation needs to be determined.\n\nTo find the value of the fifth equation, we can analyze the pattern of the equations. The first equation is a simple sum, the second line is a sum of squares, the third line is a sum of cubes, and the fourth line is a sum of fourth powers. We can use this pattern to infer the value of the fifth line. Unfortunately, without additional information or context, it is not possible to determine the value of the fifth equation based solely on the pattern of the previous lines. Therefore, the value of the fifth equation cannot be determined with the given information.\n\n**Answer:**\nThe value of the fifth equation cannot be determined with the given information."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# import os\n",
        "# import base64\n",
        "# from groq import Groq\n",
        "# import markdown\n",
        "# from IPython.display import display, Latex\n",
        "\n",
        "\n",
        "# # Set up the API client\n",
        "# client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "# # Function to encode the image to base64\n",
        "# def encode_image(image_path):\n",
        "#     with open(image_path, \"rb\") as image_file:\n",
        "#         return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# # Path to image\n",
        "# image_path = \"math.jpg\"\n",
        "# base64_image = encode_image(image_path)\n",
        "\n",
        "# # Sending image and text as a single user message\n",
        "# chat_completion = client.chat.completions.create(\n",
        "#     model=\"llama-3.2-11b-vision-preview\",\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": [\n",
        "#                 {\"type\": \"text\", \"text\": \"What is this image?\"},\n",
        "\n",
        "\n",
        "#                 {\n",
        "#                     \"type\": \"image_url\",\n",
        "#                     \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
        "#                 }\n",
        "#             ],\n",
        "#         }\n",
        "#     ],\n",
        "#     temperature=1,\n",
        "#     max_completion_tokens=2048,\n",
        "#     top_p=1,\n",
        "#     stream=False,\n",
        "#     stop=None,\n",
        "# )\n",
        "\n",
        "# # Print the response\n",
        "\n",
        "# image_description = chat_completion.choices[0].message.content\n",
        "# # print(response)\n",
        "\n",
        "# display(Markdown(image_description))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "pVwlOy9sK1ev",
        "outputId": "d8d44a74-42a6-425d-a1f6-c2e2ab5d0c40"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0mWARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
            "Started parsing the file under job_id cd2d4473-2faf-4493-a5bc-d7a7e2bf3330\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-c16b7a485490>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Process the image using LlamaParse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mparsed_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Display the parsed result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_parse/base.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, file_path, extra_info, fs)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;34m\"\"\"Load data from the input path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0masyncio_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnest_asyncio_err\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/async_utils.py\u001b[0m in \u001b[0;36masyncio_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# If we're here, there's an existing loop but it's not running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize LlamaParse with necessary configurations for image processing\n",
        "parser = LlamaParse(\n",
        "    api_key=os.getenv(\"LLAMAPARSE_API_KEY\"),  # Your LlamaParse API key\n",
        "    is_remote=True,  # Switch to remote if local is too slow\n",
        "    verbose=False,  # Turn off verbose for faster response\n",
        "    show_progress=False,  # Disable progress reporting to reduce overhead\n",
        "    language=\"en\",\n",
        "    result_type=\"markdown\",\n",
        "    max_timeout=10,  # Set a very short timeout (in seconds)\n",
        "    num_workers=4,  # Use more workers to speed up processing\n",
        "    content_guideline_instruction=(\n",
        "        \"Extract any visible math expressions from the image and describe any objects detected. \"\n",
        "        \"Focus on identifying and converting math problems to text.\"\n",
        "    ),\n",
        "    structured_output=False,  # No need for structured output, plain text is fine\n",
        "    disable_ocr=True,  # Disable OCR if there are no non-standard texts\n",
        "    extract_charts=False,  # No need to extract charts for homework screenshots\n",
        "    premium_mode=True,  # Use premium mode for optimized accuracy\n",
        ")\n",
        "\n",
        "# Path to the image\n",
        "image_path = \"math.jpg\"\n",
        "\n",
        "# Process the image using LlamaParse\n",
        "parsed_result = parser.load_data(image_path)\n",
        "\n",
        "# Display the parsed result\n",
        "print(parsed_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdwuB4hLMdm0",
        "outputId": "24a745f5-e244-49bd-c27c-2632975fda55"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
            "[Document(id_='5a3d0615-a71d-4379-85bd-29a9021573fb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='$$\\na + b + c = 4\\n$$\\n\\n$$\\na^2 + b^2 + c^2 = 10\\n$$\\n\\n$$\\na^3 + b^3 + c^3 = 22\\n$$\\n\\n$$\\na^4 + b^4 + c^4 = ?\\n$$', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming parsed_result is a list with one element (the Document object)\n",
        "document = parsed_result[0]\n",
        "\n",
        "# Extract the text from the text_resource field\n",
        "image_description = document.text_resource.text\n",
        "\n",
        "# Print the image description\n",
        "print(image_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujO9Ls2xNRaR",
        "outputId": "be779d9f-4faa-4246-f7d2-d119ed14a95a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$$\n",
            "a + b + c = 4\n",
            "$$\n",
            "\n",
            "$$\n",
            "a^2 + b^2 + c^2 = 10\n",
            "$$\n",
            "\n",
            "$$\n",
            "a^3 + b^3 + c^3 = 22\n",
            "$$\n",
            "\n",
            "$$\n",
            "a^4 + b^4 + c^4 = ?\n",
            "$$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second model: Use the description and the user prompt to generate a detailed response\n",
        "user_prompt = \"What is the result of the math equation in the image?\"\n",
        "\n",
        "# Send to second model\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": f\"Based on this image data: {image_description}, answer the question: {user_prompt}. Respond in a detailed manner using LaTeX for equations and Markdown for formatting. Ensure that all math is properly formatted and equations are clear and concise.\"},\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=2048,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "# Get and display the response from the second model\n",
        "response = chat_completion.choices[0].message.content\n",
        "\n",
        "# Now, display the response in a formatted way using Markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(response))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N3NnxY505RgN",
        "outputId": "167d9730-3431-429f-8156-1b3ef245c6db"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Step 1: Recognize the problem involves a system of equations with powers of a, b, and c.\nWe have a system of equations with $a$, $b$, and $c$ raised to various powers:\n- $a + b + c = 4$\n- $a^2 + b^2 + c^2 = 10$\n- $a^3 + b^3 + c^3 = 22$\nWe need to find $a^4 + b^4 + c^4$.\n\n## Step 2: Recall Newton's Sums to find a relationship between the sums of powers.\nNewton's Sums provide a way to express sums of powers in terms of elementary symmetric polynomials. For three variables $a$, $b$, and $c$, we define:\n- $S_1 = a + b + c$\n- $S_2 = a^2 + b^2 + c^2$\n- $S_3 = a^3 + b^3 + c^3$\n- $S_4 = a^4 + b^4 + c^4$\nAnd the elementary symmetric polynomials are:\n- $e_1 = a + b + c$\n- $e_2 = ab + bc + ca$\n- $e_3 = abc$\n\n## Step 3: Use Newton's Identities to relate $S_k$ to $e_i$.\nNewton's Identities state:\n- $S_1 = e_1$\n- $S_2 = e_1 S_1 - 2e_2$\n- $S_3 = e_1 S_2 - e_2 S_1 + 3e_3$\n- $S_4 = e_1 S_3 - e_2 S_2 + e_3 S_1$\nGiven $S_1 = 4$, $S_2 = 10$, and $S_3 = 22$, we need $e_2$ and $e_3$ to proceed.\n\n## Step 4: Calculate $e_2$ using $S_1$ and $S_2$.\nWe know $S_2 = e_1 S_1 - 2e_2$. Substituting known values:\n$$\n10 = 4 \\cdot 4 - 2e_2\n$$\n$$\n10 = 16 - 2e_2\n$$\n$$\n2e_2 = 16 - 10\n$$\n$$\n2e_2 = 6\n$$\n$$\ne_2 = 3\n$$\n\n## Step 5: Calculate $e_3$ using $S_1$, $S_2$, and $S_3$.\nFrom $S_3 = e_1 S_2 - e_2 S_1 + 3e_3$:\n$$\n22 = 4 \\cdot 10 - 3 \\cdot 4 + 3e_3\n$$\n$$\n22 = 40 - 12 + 3e_3\n$$\n$$\n22 = 28 + 3e_3\n$$\n$$\n3e_3 = 22 - 28\n$$\n$$\n3e_3 = -6\n$$\n$$\ne_3 = -2\n$$\n\n## Step 6: Find $S_4$ using Newton's Identity for $S_4$.\nNow, calculate $S_4 = e_1 S_3 - e_2 S_2 + e_3 S_1$:\n$$\nS_4 = 4 \\cdot 22 - 3 \\cdot 10 + (-2) \\cdot 4\n$$\n$$\nS_4 = 88 - 30 - 8\n$$\n$$\nS_4 = 50\n$$\n\nThe final answer is: $\\boxed{50}$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set up the second agent's message\n",
        "# formatting_agent_message = f\"\"\"\n",
        "# You are an AI agent designed to fix formatting in model responses. The model has provided the following response, but it needs formatting corrections.\n",
        "\n",
        "# Here is the response:\n",
        "# {response}\n",
        "\n",
        "# Your task is to identify the response and format it correctly. If there is math equations, use Latex. Use Markdown elsewhere.\n",
        "# \"\"\"\n",
        "\n",
        "# # Send the LaTeX response to the formatting agent\n",
        "# formatting_completion = client.chat.completions.create(\n",
        "#     model=\"llama-3.2-11b-vision-preview\",  # Or any other suitable model\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": [\n",
        "#                 {\"type\": \"text\", \"text\": formatting_agent_message}\n",
        "#             ]\n",
        "#         }\n",
        "#     ],\n",
        "#     temperature=1,\n",
        "#     max_completion_tokens=2048,\n",
        "#     top_p=1,\n",
        "#     stream=False,\n",
        "#     stop=None,\n",
        "# )\n",
        "\n",
        "# # Get the corrected response\n",
        "# corrected_response = formatting_completion.choices[0].message.content\n",
        "\n",
        "# # Display the corrected LaTeX in Markdown format\n",
        "# # display(Markdown(corrected_response))\n",
        "# display(Latex(corrected_response))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SQOQ23Ywyl9Q",
        "outputId": "03acaa0b-0864-4b33-9d77-8ec5ba789bbd"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "**Problem-Solving with the Attached Image**\n================================================\n\nNotably, the attached image depicts a single pickle.\n\n### Solution Approach\n\nThere is no apparent mathematical or computational problem to solve in this image, as it represents a non-ambiguous visual representation of a standard pickle (likely a dill pickle). It does not elicit a specific task or problem to be resolved, and its purpose appears to be illustrating or showcasing a pickle.\n\nNote: Since there are no math equations in this response, I have used Markdown formatting as required."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1A3ZIPUbqQaV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}