{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkLtGavrQm7qfg7iBdnDBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "614408574371450a8e1c37d811a53eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Image Path:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9babeb1ac8e84adb96d4fb0ee0e6f76e",
            "placeholder": "Enter image path",
            "style": "IPY_MODEL_42a0da23afdd41638af029e20f8a9680",
            "value": "math.jpg"
          }
        },
        "9babeb1ac8e84adb96d4fb0ee0e6f76e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a0da23afdd41638af029e20f8a9680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09e0cc5b0b6d41e6b5951f382661d82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Prompt:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_89c1abc96d4244af81c96c9f39ac3996",
            "placeholder": "Enter your prompt",
            "rows": null,
            "style": "IPY_MODEL_8c18c49c89544e768a58bc234c4fdeed",
            "value": "What is the result of the math equation in the image?"
          }
        },
        "89c1abc96d4244af81c96c9f39ac3996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c18c49c89544e768a58bc234c4fdeed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e12b584579434d36a7ae6b83fd996a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_713c43becd7246f0b9618d9b254ea99d",
            "style": "IPY_MODEL_dc933bb849d641439fcdbeba172a1ebf",
            "tooltip": ""
          }
        },
        "713c43becd7246f0b9618d9b254ea99d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc933bb849d641439fcdbeba172a1ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "9f6a06fb47134384b9a256a2d32bc084": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_5b33cd537d79462b89d9d12d28da1e1c",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "IMAGE DESCR: $$a + b + c = 4$$\n",
                  "\n",
                  "$$a^2 + b^2 + c^2 = 10$$\n",
                  "\n",
                  "$$a^3 + b^3 + c^3 = 22$$\n",
                  "\n",
                  "$$a^4 + b^4 + c^4 = ?$$\n",
                  "-----\n",
                  "\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Markdown object>",
                  "text/markdown": "## Step 1: Recognize the pattern of the equations\nThe equations given are in the form of sums of powers of $a$, $b$, and $c$. We have equations for the sum of the first power, square, cube, and we need to find the sum of the fourth power.\n\n## Step 2: Use Newton's Sums formula to derive relationships\nNewton's Sums relate the sums of powers of the roots of a polynomial to its coefficients. We can use these formulas to derive relationships between $a+b+c$, $a^2+b^2+c^2$, $a^3+b^3+c^3$, and $a^4+b^4+c^4$. Newton's Sums formulas for the first few powers are:\n- $P_1 = a + b + c$\n- $P_2 = a^2 + b^2 + c^2 = (a + b + c)^2 - 2(ab + ac + bc)$\n- $P_3 = a^3 + b^3 + c^3 = (a + b + c)^3 - 3(a + b + c)(ab + ac + bc) + 3abc$\n- $P_4 = a^4 + b^4 + c^4 = (a + b + c)^4 - 4(a + b + c)^2(ab + ac + bc) + 2(ab + ac + bc)^2 + 4(a + b + c)abc$\n\n## Step 3: Calculate $(ab + ac + bc)$ using $P_1$ and $P_2$\nFrom $P_1 = 4$ and $P_2 = 10$, we can find $(ab + ac + bc)$. We know that $P_2 = (a + b + c)^2 - 2(ab + ac + bc)$. Substituting the given values, $10 = 4^2 - 2(ab + ac + bc)$, which simplifies to $10 = 16 - 2(ab + ac + bc)$. Solving for $(ab + ac + bc)$, we get $2(ab + ac + bc) = 16 - 10 = 6$, thus $(ab + ac + bc) = 3$.\n\n## Step 4: Calculate $abc$ using $P_1$, $P_2$, $P_3$, and the derived $(ab + ac + bc)$\nUsing $P_3 = (a + b + c)^3 - 3(a + b + c)(ab + ac + bc) + 3abc$, we substitute the known values: $22 = 4^3 - 3(4)(3) + 3abc$. This simplifies to $22 = 64 - 36 + 3abc$, which further simplifies to $22 = 28 + 3abc$. Solving for $abc$, $3abc = 22 - 28 = -6$, thus $abc = -2$.\n\n## Step 5: Calculate $a^4 + b^4 + c^4$ using Newton's Sums formula for $P_4$\nSubstitute the known values into the formula for $P_4$: $a^4 + b^4 + c^4 = (4)^4 - 4(4)^2(3) + 2(3)^2 + 4(4)(-2)$. This simplifies to $a^4 + b^4 + c^4 = 256 - 192 + 18 - 32$.\n\n## Step 6: Perform the arithmetic to find $a^4 + b^4 + c^4$\nCalculate the value: $256 - 192 + 18 - 32 = 50$.\n\nThe final answer is: $\\boxed{50}$"
                },
                "metadata": {}
              }
            ]
          }
        },
        "5b33cd537d79462b89d9d12d28da1e1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etuckerman/SPRK/blob/main/groq_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Store API keys as a secret in Colab userdata\n",
        "api_key = '----------------'\n",
        "secret_name = 'GROQ_API_KEY'\n",
        "\n",
        "# Set the API key directly as an environment variable\n",
        "os.environ[\"GROQ_API_KEY\"] = api_key\n",
        "#########################################################################\n",
        "api_key = '-----------------'\n",
        "secret_name = 'LLAMAPARSE_API_KEY'\n",
        "\n",
        "# Set the API key directly as an environment variable\n",
        "os.environ[\"LLAMAPARSE_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "vvFuLDd0tEHd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq markdown Latex llama_parse nest_asyncio gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbNDT7lgq9CD",
        "outputId": "2fa6df31-a241-40e2-c4ad-99b1167a17f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (3.7)\n",
            "Collecting Latex\n",
            "  Downloading latex-0.7.0.tar.gz (6.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting llama_parse\n",
            "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.14.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Collecting tempdir (from Latex)\n",
            "  Downloading tempdir-0.7.1.tar.gz (5.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting data (from Latex)\n",
            "  Downloading data-0.4.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from Latex) (1.0.0)\n",
            "Collecting shutilwhich (from Latex)\n",
            "  Downloading shutilwhich-1.1.0.tar.gz (2.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama_parse) (8.1.8)\n",
            "Collecting llama-index-core>=0.11.0 (from llama_parse)\n",
            "  Downloading llama_index_core-0.12.15-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.0 (from gradio)\n",
            "  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama_parse) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (3.11.11)\n",
            "Collecting dataclasses-json (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (3.9.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core>=0.11.0->llama_parse) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from data->Latex) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from data->Latex) (4.4.2)\n",
            "Collecting funcsigs (from data->Latex)\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_parse) (1.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama_parse) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama_parse) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama_parse) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core>=0.11.0->llama_parse)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading groq-0.16.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.20-py3-none-any.whl (16 kB)\n",
            "Downloading gradio-5.14.0-py3-none-any.whl (57.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_core-0.12.15-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: Latex, data, shutilwhich, tempdir\n",
            "  Building wheel for Latex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Latex: filename=latex-0.7.0-py3-none-any.whl size=7588 sha256=3638067973be55990806b1a5b54a3274fd75b51bfd4ea84ad794d93f574955a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/b3/95/f4b45b5116d4585893cdcb2ac7c07614a59fb047c754c4651a\n",
            "  Building wheel for data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7227 sha256=df366de762abd8e3d94ed0cc15dc1c59920d8ccc36552e05ca6a209f75e967dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/d3/10/d5fe9bc9dcb197ea289baccca92a25f2f95135235a92ca1b11\n",
            "  Building wheel for shutilwhich (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shutilwhich: filename=shutilwhich-1.1.0-py3-none-any.whl size=2766 sha256=7234ccb9fd458152838fbc455c1b8ad20dd162cbcde77c5bc0822120ef73cfc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/49/7a/3997889a5643ddb4a1d21692c6916fd2fc482965211d9a3ca5\n",
            "  Building wheel for tempdir (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tempdir: filename=tempdir-0.7.1-py3-none-any.whl size=2196 sha256=92cd1f8d9968f99b04175381ad66878da5b16bcd29b748b600387c1698477625\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/67/0b/519393cca63cd89cea554a371cc88a4c439c2d31804e9a9ed7\n",
            "Successfully built Latex data shutilwhich tempdir\n",
            "Installing collected packages: tempdir, shutilwhich, pydub, funcsigs, filetype, dirtyjson, uvicorn, tomlkit, semantic-version, ruff, python-multipart, mypy-extensions, marshmallow, markupsafe, ffmpy, data, aiofiles, typing-inspect, tiktoken, starlette, Latex, safehttpx, groq, gradio-client, fastapi, dataclasses-json, llama-index-core, gradio, llama_parse\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Latex-0.7.0 aiofiles-23.2.1 data-0.4 dataclasses-json-0.6.7 dirtyjson-1.0.8 fastapi-0.115.8 ffmpy-0.5.0 filetype-1.2.0 funcsigs-1.0.2 gradio-5.14.0 gradio-client-1.7.0 groq-0.16.0 llama-index-core-0.12.15 llama_parse-0.5.20 markupsafe-2.1.5 marshmallow-3.26.1 mypy-extensions-1.0.0 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.4 safehttpx-0.1.6 semantic-version-2.10.0 shutilwhich-1.1.0 starlette-0.45.3 tempdir-0.7.1 tiktoken-0.8.0 tomlkit-0.13.2 typing-inspect-0.9.0 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.models.list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MivE20JsXrS",
        "outputId": "70602499-bd59-459e-9730-4f9d1d4c7dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelListResponse(data=[Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None), Model(id='llama-3.2-3b-preview', created=1727224290, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='mixtral-8x7b-32768', created=1693721698, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None), Model(id='llama-3.2-11b-vision-preview', created=1727226869, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='llama-3.2-90b-vision-preview', created=1727226914, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.2-1b-preview', created=1727224268, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-specdec', created=1733505017, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=32768, public_apps=None), Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None)], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "LGhef7yLqA-8",
        "outputId": "67f78ee5-d646-4a2b-c474-5239db8b171c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The image is a mathematical equation set, consisting of five lines of text in a black font, each containing a variable and a constant. The first line reads \"a + b + c = 4\", the second line reads \"a^2 + b^2 + c^2 = 10\", the third line reads \"a^3 + b^3 + c^3 = 22\", the fourth line reads \"a^4 + b^4 + c^4 = ?\", and the fifth line is a question mark, indicating that the value of this equation needs to be determined.\n\nTo find the value of the fifth equation, we can analyze the pattern of the equations. The first equation is a simple sum, the second line is a sum of squares, the third line is a sum of cubes, and the fourth line is a sum of fourth powers. We can use this pattern to infer the value of the fifth line. Unfortunately, without additional information or context, it is not possible to determine the value of the fifth equation based solely on the pattern of the previous lines. Therefore, the value of the fifth equation cannot be determined with the given information.\n\n**Answer:**\nThe value of the fifth equation cannot be determined with the given information."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# import os\n",
        "# import base64\n",
        "# from groq import Groq\n",
        "# import markdown\n",
        "# from IPython.display import display, Latex\n",
        "\n",
        "\n",
        "# # Set up the API client\n",
        "# client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "# # Function to encode the image to base64\n",
        "# def encode_image(image_path):\n",
        "#     with open(image_path, \"rb\") as image_file:\n",
        "#         return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# # Path to image\n",
        "# image_path = \"math.jpg\"\n",
        "# base64_image = encode_image(image_path)\n",
        "\n",
        "# # Sending image and text as a single user message\n",
        "# chat_completion = client.chat.completions.create(\n",
        "#     model=\"llama-3.2-11b-vision-preview\",\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": [\n",
        "#                 {\"type\": \"text\", \"text\": \"What is this image?\"},\n",
        "\n",
        "\n",
        "#                 {\n",
        "#                     \"type\": \"image_url\",\n",
        "#                     \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
        "#                 }\n",
        "#             ],\n",
        "#         }\n",
        "#     ],\n",
        "#     temperature=1,\n",
        "#     max_completion_tokens=2048,\n",
        "#     top_p=1,\n",
        "#     stream=False,\n",
        "#     stop=None,\n",
        "# )\n",
        "\n",
        "# # Print the response\n",
        "\n",
        "# image_description = chat_completion.choices[0].message.content\n",
        "# # print(response)\n",
        "\n",
        "# display(Markdown(image_description))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_parse import LlamaParse\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "pVwlOy9sK1ev"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize LlamaParse with necessary configurations for image processing\n",
        "parser = LlamaParse(\n",
        "    api_key=os.getenv(\"LLAMAPARSE_API_KEY\"),  # Your LlamaParse API key\n",
        "    is_remote=True,  # Switch to remote if local is too slow\n",
        "    verbose=False,  # Turn off verbose for faster response\n",
        "    show_progress=False,  # Disable progress reporting to reduce overhead\n",
        "    language=\"en\",\n",
        "    result_type=\"markdown\",\n",
        "    max_timeout=10,  # Set a very short timeout (in seconds)\n",
        "    num_workers=4,  # Use more workers to speed up processing\n",
        "    content_guideline_instruction=(\n",
        "        \"Extract any visible math expressions from the image and describe any objects detected. \"\n",
        "        \"Focus on identifying and converting math problems to text.\"\n",
        "    ),\n",
        "    structured_output=False,  # No need for structured output, plain text is fine\n",
        "    disable_ocr=True,  # Disable OCR if there are no non-standard texts\n",
        "    extract_charts=False,  # No need to extract charts for homework screenshots\n",
        "    premium_mode=True,  # Use premium mode for optimized accuracy\n",
        ")\n",
        "\n",
        "# Path to the image\n",
        "image_path = \"math.jpg\"\n",
        "\n",
        "# Process the image using LlamaParse\n",
        "parsed_result = parser.load_data(image_path)\n",
        "\n",
        "# Display the parsed result\n",
        "print(parsed_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdwuB4hLMdm0",
        "outputId": "24a745f5-e244-49bd-c27c-2632975fda55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
            "[Document(id_='5a3d0615-a71d-4379-85bd-29a9021573fb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='$$\\na + b + c = 4\\n$$\\n\\n$$\\na^2 + b^2 + c^2 = 10\\n$$\\n\\n$$\\na^3 + b^3 + c^3 = 22\\n$$\\n\\n$$\\na^4 + b^4 + c^4 = ?\\n$$', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming parsed_result is a list with one element (the Document object)\n",
        "document = parsed_result[0]\n",
        "\n",
        "# Extract the text from the text_resource field\n",
        "image_description = document.text_resource.text\n",
        "\n",
        "# Print the image description\n",
        "print(image_description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujO9Ls2xNRaR",
        "outputId": "4db2049a-9841-4f57-e982-8b075a95f26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$$\n",
            "a + b + c = 4\n",
            "$$\n",
            "\n",
            "$$\n",
            "a^2 + b^2 + c^2 = 10\n",
            "$$\n",
            "\n",
            "$$\n",
            "a^3 + b^3 + c^3 = 22\n",
            "$$\n",
            "\n",
            "$$\n",
            "a^4 + b^4 + c^4 = ?\n",
            "$$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second model: Use the description and the user prompt to generate a detailed response\n",
        "user_prompt = \"What is the result of the math equation in the image?\"\n",
        "\n",
        "\n",
        "# Send to second model\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": f\"Based on this image data: {image_description}, answer the question: {user_prompt}. Respond in a detailed manner using LaTeX for equations and Markdown for formatting. Ensure that all math is properly formatted and equations are clear and concise.\"},\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_completion_tokens=2048,\n",
        "    top_p=1,\n",
        "    stream=False,\n",
        "    stop=None,\n",
        ")\n",
        "\n",
        "# Get and display the response from the second model\n",
        "response = chat_completion.choices[0].message.content\n",
        "\n",
        "# Now, display the response in a formatted way using Markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(response))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N3NnxY505RgN",
        "outputId": "167d9730-3431-429f-8156-1b3ef245c6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Step 1: Recognize the problem involves a system of equations with powers of a, b, and c.\nWe have a system of equations with $a$, $b$, and $c$ raised to various powers:\n- $a + b + c = 4$\n- $a^2 + b^2 + c^2 = 10$\n- $a^3 + b^3 + c^3 = 22$\nWe need to find $a^4 + b^4 + c^4$.\n\n## Step 2: Recall Newton's Sums to find a relationship between the sums of powers.\nNewton's Sums provide a way to express sums of powers in terms of elementary symmetric polynomials. For three variables $a$, $b$, and $c$, we define:\n- $S_1 = a + b + c$\n- $S_2 = a^2 + b^2 + c^2$\n- $S_3 = a^3 + b^3 + c^3$\n- $S_4 = a^4 + b^4 + c^4$\nAnd the elementary symmetric polynomials are:\n- $e_1 = a + b + c$\n- $e_2 = ab + bc + ca$\n- $e_3 = abc$\n\n## Step 3: Use Newton's Identities to relate $S_k$ to $e_i$.\nNewton's Identities state:\n- $S_1 = e_1$\n- $S_2 = e_1 S_1 - 2e_2$\n- $S_3 = e_1 S_2 - e_2 S_1 + 3e_3$\n- $S_4 = e_1 S_3 - e_2 S_2 + e_3 S_1$\nGiven $S_1 = 4$, $S_2 = 10$, and $S_3 = 22$, we need $e_2$ and $e_3$ to proceed.\n\n## Step 4: Calculate $e_2$ using $S_1$ and $S_2$.\nWe know $S_2 = e_1 S_1 - 2e_2$. Substituting known values:\n$$\n10 = 4 \\cdot 4 - 2e_2\n$$\n$$\n10 = 16 - 2e_2\n$$\n$$\n2e_2 = 16 - 10\n$$\n$$\n2e_2 = 6\n$$\n$$\ne_2 = 3\n$$\n\n## Step 5: Calculate $e_3$ using $S_1$, $S_2$, and $S_3$.\nFrom $S_3 = e_1 S_2 - e_2 S_1 + 3e_3$:\n$$\n22 = 4 \\cdot 10 - 3 \\cdot 4 + 3e_3\n$$\n$$\n22 = 40 - 12 + 3e_3\n$$\n$$\n22 = 28 + 3e_3\n$$\n$$\n3e_3 = 22 - 28\n$$\n$$\n3e_3 = -6\n$$\n$$\ne_3 = -2\n$$\n\n## Step 6: Find $S_4$ using Newton's Identity for $S_4$.\nNow, calculate $S_4 = e_1 S_3 - e_2 S_2 + e_3 S_1$:\n$$\nS_4 = 4 \\cdot 22 - 3 \\cdot 10 + (-2) \\cdot 4\n$$\n$$\nS_4 = 88 - 30 - 8\n$$\n$$\nS_4 = 50\n$$\n\nThe final answer is: $\\boxed{50}$"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from llama_parse import LlamaParse\n",
        "from IPython.display import display, Markdown\n",
        "from groq import Groq\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "def call_llm(image_path, user_prompt):\n",
        "    # Initialize LlamaParse with necessary configurations for image processing\n",
        "    parser = LlamaParse(\n",
        "        api_key=os.getenv(\"LLAMAPARSE_API_KEY\"),  # Your LlamaParse API key\n",
        "        is_remote=False,  # Switch to remote if local is too slow\n",
        "        verbose=False,  # Turn off verbose for faster response\n",
        "        show_progress=True,  # Disable progress reporting to reduce overhead\n",
        "        language=\"en\",\n",
        "        result_type=\"markdown\",\n",
        "        max_timeout=10,  # Set a very short timeout (in seconds)\n",
        "        num_workers=4,  # Use more workers to speed up processing\n",
        "        content_guideline_instruction=f\"Extract the text if present; otherwise, describe the image.\",\n",
        "\n",
        "        structured_output=False,  # No need for structured output, plain text is fine\n",
        "        disable_ocr=False,  # Disable OCR if there are no non-standard texts\n",
        "        extract_charts=True,  # No need for extracting charts for homework screenshots\n",
        "        premium_mode=True,  # Use premium mode for optimized accuracy\n",
        "    )\n",
        "\n",
        "    # Process the image using LlamaParse\n",
        "    parsed_result = parser.load_data(image_path)\n",
        "\n",
        "    # Extract the text from the text_resource field\n",
        "    image_description = parsed_result[0].text_resource.text\n",
        "    print(f\"IMAGE DESCR: {image_description}\\n-----\\n\")\n",
        "    # Send the image description and user prompt to the second model\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": f\"Based on this image data: {image_description}, respond to this prompt: {user_prompt}.\"},\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        temperature=1,\n",
        "        max_completion_tokens=1024,\n",
        "        top_p=1,\n",
        "        stream=False,\n",
        "        stop=None,\n",
        "    )\n",
        "\n",
        "    # Get and display the response from the second model\n",
        "    response = chat_completion.choices[0].message.content\n",
        "\n",
        "    # Display the response in a formatted way using Markdown\n",
        "    display(Markdown(response))\n",
        "\n",
        "# # Example usage\n",
        "# image_path = \"flyer.jpg\"\n",
        "# user_prompt = \"i want to go\"\n",
        "\n",
        "# call_llm(image_path, user_prompt)\n"
      ],
      "metadata": {
        "id": "r5Iop92ds-Jn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a seperate tab which is a frontend UI for the call_llm() function above. not gradio.\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Create input widgets\n",
        "image_path_widget = widgets.Text(\n",
        "    value=\"math.jpg\",  # Default value\n",
        "    placeholder=\"Enter image path\",\n",
        "    description=\"Image Path:\",\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "user_prompt_widget = widgets.Textarea(\n",
        "    value=\"What is the result of the math equation in the image?\",  # Default value\n",
        "    placeholder=\"Enter your prompt\",\n",
        "    description=\"Prompt:\",\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "submit_button = widgets.Button(description=\"Submit\")\n",
        "\n",
        "# Output widget to display the results\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "# Function to handle button click\n",
        "def on_submit_clicked(b):\n",
        "    with output_widget:\n",
        "        clear_output()  # Clear previous output\n",
        "        image_path = image_path_widget.value\n",
        "        user_prompt = user_prompt_widget.value\n",
        "        try:\n",
        "            call_llm(image_path, user_prompt)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "submit_button.on_click(on_submit_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "display(image_path_widget)\n",
        "display(user_prompt_widget)\n",
        "display(submit_button)\n",
        "display(output_widget)\n"
      ],
      "metadata": {
        "id": "C-mkzZ4xVQGO",
        "outputId": "5a51ff4a-b165-410f-b878-d17744159517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "614408574371450a8e1c37d811a53eea",
            "9babeb1ac8e84adb96d4fb0ee0e6f76e",
            "42a0da23afdd41638af029e20f8a9680",
            "09e0cc5b0b6d41e6b5951f382661d82f",
            "89c1abc96d4244af81c96c9f39ac3996",
            "8c18c49c89544e768a58bc234c4fdeed",
            "e12b584579434d36a7ae6b83fd996a79",
            "713c43becd7246f0b9618d9b254ea99d",
            "dc933bb849d641439fcdbeba172a1ebf",
            "9f6a06fb47134384b9a256a2d32bc084",
            "5b33cd537d79462b89d9d12d28da1e1c"
          ]
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='math.jpg', description='Image Path:', placeholder='Enter image path')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "614408574371450a8e1c37d811a53eea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='What is the result of the math equation in the image?', description='Prompt:', placeholder='En…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09e0cc5b0b6d41e6b5951f382661d82f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Submit', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e12b584579434d36a7ae6b83fd996a79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f6a06fb47134384b9a256a2d32bc084"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set up the second agent's message\n",
        "# formatting_agent_message = f\"\"\"\n",
        "# You are an AI agent designed to fix formatting in model responses. The model has provided the following response, but it needs formatting corrections.\n",
        "\n",
        "# Here is the response:\n",
        "# {response}\n",
        "\n",
        "# Your task is to identify the response and format it correctly. If there is math equations, use Latex. Use Markdown elsewhere.\n",
        "# \"\"\"\n",
        "\n",
        "# # Send the LaTeX response to the formatting agent\n",
        "# formatting_completion = client.chat.completions.create(\n",
        "#     model=\"llama-3.2-11b-vision-preview\",  # Or any other suitable model\n",
        "#     messages=[\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": [\n",
        "#                 {\"type\": \"text\", \"text\": formatting_agent_message}\n",
        "#             ]\n",
        "#         }\n",
        "#     ],\n",
        "#     temperature=1,\n",
        "#     max_completion_tokens=2048,\n",
        "#     top_p=1,\n",
        "#     stream=False,\n",
        "#     stop=None,\n",
        "# )\n",
        "\n",
        "# # Get the corrected response\n",
        "# corrected_response = formatting_completion.choices[0].message.content\n",
        "\n",
        "# # Display the corrected LaTeX in Markdown format\n",
        "# # display(Markdown(corrected_response))\n",
        "# display(Latex(corrected_response))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SQOQ23Ywyl9Q",
        "outputId": "03acaa0b-0864-4b33-9d77-8ec5ba789bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "**Problem-Solving with the Attached Image**\n================================================\n\nNotably, the attached image depicts a single pickle.\n\n### Solution Approach\n\nThere is no apparent mathematical or computational problem to solve in this image, as it represents a non-ambiguous visual representation of a standard pickle (likely a dill pickle). It does not elicit a specific task or problem to be resolved, and its purpose appears to be illustrating or showcasing a pickle.\n\nNote: Since there are no math equations in this response, I have used Markdown formatting as required."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from llama_parse import LlamaParse\n",
        "# from IPython.display import display, Markdown\n",
        "# from groq import Groq\n",
        "# client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "# def call_llm(image_path, user_prompt):\n",
        "#     # Initialize LlamaParse with necessary configurations for image processing\n",
        "#     parser = LlamaParse(\n",
        "#         api_key=os.getenv(\"LLAMAPARSE_API_KEY\"),  # Your LlamaParse API key\n",
        "#         is_remote=False,  # Switch to remote if local is too slow\n",
        "#         verbose=False,  # Turn off verbose for faster response\n",
        "#         show_progress=False,  # Disable progress reporting to reduce overhead\n",
        "#         language=\"en\",\n",
        "#         result_type=\"markdown\",\n",
        "#         max_timeout=100,  # Set a very short timeout (in seconds)\n",
        "#         num_workers=4,  # Use more workers to speed up processing\n",
        "#         complemental_formatting_instruction=(\n",
        "#             \"Describe the image.\"\n",
        "#         ),\n",
        "#         structured_output=False,  # No need for structured output, plain text is fine\n",
        "#         disable_ocr=False,  # Disable OCR if there are no non-standard texts\n",
        "#         extract_charts=True,  # No need for extracting charts for homework screenshots\n",
        "#         premium_mode=True,  # Use premium mode for optimized accuracy\n",
        "#     )\n",
        "\n",
        "#     # Process the image using LlamaParse\n",
        "#     parsed_result = parser.load_data(image_path)\n",
        "\n",
        "#     # Extract the text from the text_resource field\n",
        "#     image_description = parsed_result[0].text_resource.text\n",
        "#     print(f\"IMAGE DESCR: {image_description}\\n-----\\n\")\n",
        "#     # Send the image description and user prompt to the second model\n",
        "#     chat_completion = client.chat.completions.create(\n",
        "#         model=\"llama-3.3-70b-versatile\",\n",
        "#         messages=[\n",
        "#             {\n",
        "#                 \"role\": \"user\",\n",
        "#                 \"content\": [\n",
        "#                     {\"type\": \"text\", \"text\": f\"Based on this image data: {image_description}, respond to this prompt: {user_prompt}.\"},\n",
        "#                 ],\n",
        "#             }\n",
        "#         ],\n",
        "#         temperature=1,\n",
        "#         max_completion_tokens=1024,\n",
        "#         top_p=1,\n",
        "#         stream=False,\n",
        "#         stop=None,\n",
        "#     )\n",
        "\n",
        "#     # Get and display the response from the second model\n",
        "#     response = chat_completion.choices[0].message.content\n",
        "\n",
        "#     # Display the response in a formatted way using Markdown\n",
        "#     display(Markdown(response))\n",
        "\n",
        "# # Example usage\n",
        "# image_path = \"math.jpg\"\n",
        "# user_prompt = \"help\"\n",
        "\n",
        "# call_llm(image_path, user_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "1A3ZIPUbqQaV",
        "outputId": "0b90497a-63b1-4398-d0a1-14e1274776a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
            "Error while parsing the file 'math.jpg': Timeout while parsing the file: bfb80903-cc8f-4450-a656-efa5342057b0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-eba0f991388e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0muser_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"help\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mcall_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-188-eba0f991388e>\u001b[0m in \u001b[0;36mcall_llm\u001b[0;34m(image_path, user_prompt)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Extract the text from the text_resource field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mimage_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"IMAGE DESCR: {image_description}\\n-----\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Send the image description and user prompt to the second model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "A79TVDk7XpqF",
        "outputId": "a14e3489-5294-4d44-d7ec-aeb3794af223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'gradio' has no attribute 'inputs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-156-885e6e17b4c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m interface = gr.Interface(\n\u001b[1;32m     65\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_chat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"User Prompt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mlive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m  \u001b[0;31m# Allows continuous updates during the conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gradio' has no attribute 'inputs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import tempfile\n",
        "# from llama_parse import LlamaParse\n",
        "# import gradio as gr\n",
        "# from groq import Groq\n",
        "\n",
        "# # Initialize Groq client with your API key\n",
        "# client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "# # A global variable to track the conversation history\n",
        "# conversation_history = []\n",
        "\n",
        "# def call_llm(user_prompt, image=None):\n",
        "#     global conversation_history\n",
        "#     print(\"Starting conversation...\")\n",
        "\n",
        "#     try:\n",
        "#         image_path = None\n",
        "#         image_description = None  # Initialize the image description\n",
        "\n",
        "#         if image:\n",
        "#             # Save the image to a temporary file\n",
        "#             with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as temp_file:\n",
        "#                 image.save(temp_file, format=\"JPEG\")\n",
        "#                 image_path = temp_file.name\n",
        "#                 print(f\"Image saved to: {image_path}\")\n",
        "\n",
        "#             # Process the image using LlamaParse\n",
        "#             print(\"Processing image...\")\n",
        "#             parser = LlamaParse(\n",
        "#               api_key=os.getenv(\"LLAMAPARSE_API_KEY\"),  # Your LlamaParse API key\n",
        "\n",
        "#               auto_mode=True\n",
        "#             )\n",
        "\n",
        "#             # Step 1: Process the image using LlamaParse\n",
        "#             parsed_result = parser.load_data(image_path)\n",
        "#             print(f\"Parsed result: {parsed_result}\")\n",
        "\n",
        "#             if parsed_result:\n",
        "#                 # Step 2: Extract the text from the text_resource field\n",
        "#                 image_description = parsed_result[0].text_resource.text\n",
        "#                 print(f\"Image Description: {image_description}\")\n",
        "#                 # Add the image description as part of the conversation history\n",
        "#                 conversation_history.append(f\"Image Description: {image_description}\")\n",
        "\n",
        "#         # Continue the conversation by sending the user prompt (and image description if available)\n",
        "#         conversation_history.append(f\"User: {user_prompt}\")\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"llama-3.3-70b-versatile\",\n",
        "#             messages=[{\n",
        "#                 \"role\": \"user\",\n",
        "#                 \"content\": f\"Based on this image data (if any): {image_description if image_path else 'None'}, respond to this prompt: {user_prompt}.\",\n",
        "#             }],\n",
        "#             temperature=0.7,\n",
        "#             max_completion_tokens=1024,\n",
        "#             top_p=1,\n",
        "#             stream=False,\n",
        "#         )\n",
        "\n",
        "#         # Extract the response from the Groq model\n",
        "#         chatbot_response = response.choices[0].message.content\n",
        "\n",
        "#         conversation_history.append(f\"Bot: {chatbot_response}\")\n",
        "#         return chatbot_response\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error: {e}\")\n",
        "#         return f\"Error: {e}\"\n",
        "\n",
        "# # Define Gradio interface\n",
        "# inputs = [\n",
        "#     gr.Textbox(label=\"User Prompt\", placeholder=\"Ask something...\"),\n",
        "#     gr.Image(label=\"Upload an Image (Optional)\", type=\"pil\")  # No optional argument needed here\n",
        "# ]\n",
        "# outputs = gr.Textbox(label=\"Response\")\n",
        "\n",
        "# # Enable Gradio's debug mode by setting debug=True\n",
        "# gr.Interface(fn=call_llm, inputs=inputs, outputs=outputs).launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "rop59dW9jN4s",
        "outputId": "6df80327-1bbc-41a4-e434-a551c01fc892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1e934fe5380bf930d8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1e934fe5380bf930d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting conversation...\n",
            "Starting conversation...\n",
            "Image saved to: /tmp/tmpu53x8b4b.jpg\n",
            "Processing image...\n",
            "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
            "Started parsing the file under job_id 1ba8aa34-9d5a-4e0c-90b5-e98dd65bb7d9\n",
            "......Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7865 <> https://1e934fe5380bf930d8.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9AW-_rCjckD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}